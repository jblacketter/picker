# Phase 3 Planning: Market Context & Advanced Metrics

**Date:** October 30, 2025
**Status:** ğŸ“‹ PLANNING - Pending Codex Review
**Version:** 1.0 (Draft)

---

## Table of Contents
1. [Executive Summary](#executive-summary)
2. [Phase 3 Goals](#phase-3-goals)
3. [Feature Prioritization Matrix](#feature-prioritization-matrix)
4. [Technical Architecture](#technical-architecture)
5. [Feature Implementation Plans](#feature-implementation-plans)
6. [Database Schema Changes](#database-schema-changes)
7. [API Requirements & Costs](#api-requirements--costs)
8. [UI/UX Design](#uiux-design)
9. [Testing Strategy](#testing-strategy)
10. [Risk Assessment](#risk-assessment)
11. [Timeline & Effort Estimates](#timeline--effort-estimates)
12. [Success Metrics](#success-metrics)
13. [Questions for Codex Review](#questions-for-codex-review)

---

## Executive Summary

Phase 3 aims to transform the Pre-Market Movers feature from a basic scanner into a comprehensive market intelligence tool. The focus is on adding **contextual awareness** to help users evaluate opportunities more effectively.

### Core Principle
> "A 10% pre-market mover means something very different when SPY is up 2% vs down 2%"

### What We're Building
1. **Market Context Dashboard** - Real-time overview of market conditions (indices, VIX, futures, sector performance)
2. **Advanced Volume Analysis** - VWAP comparison, volume profile, institutional activity indicators
3. **Enhanced News Intelligence** - AI-powered sentiment analysis, news clustering, catalyst categorization
4. **Performance Infrastructure** - Rate limiting, caching, retry logic to support heavier data loads

### Expected Outcomes
- **Better Trade Decisions:** Users see stocks in market context, not isolation
- **Faster Workflow:** Cached data reduces scan times from 30s â†’ 10s
- **Smarter Filtering:** AI categorizes catalysts (earnings, FDA, M&A) automatically
- **Cost Efficiency:** Caching + sentiment analysis reduces per-stock research costs by 40%

---

## Phase 3 Goals

### Primary Goals
1. âœ… **Add Market Context** - Give users situational awareness of overall market conditions
2. âœ… **Enhance Volume Analysis** - Provide deeper conviction signals beyond basic RVOL
3. âœ… **Improve News Intelligence** - Automatically categorize and prioritize news catalysts
4. âœ… **Optimize Performance** - Reduce scan times and API costs through caching/throttling

### Secondary Goals
- Reduce cognitive load (users spend less time analyzing, more time acting)
- Increase confidence in trade ideas through richer data
- Lower API costs per research session
- Prepare infrastructure for Phase 4 (user experience features)

### Non-Goals (Deferred to Phase 4+)
- âŒ Keyboard shortcuts and UX polish
- âŒ Saved filter presets
- âŒ Bulk research functionality
- âŒ CSV/JSON export
- âŒ Social sentiment (Twitter/Reddit integration)
- âŒ Technical indicators (RSI, MACD, etc.)

---

## Feature Prioritization Matrix

### Effort vs Value Analysis

| Feature | Value | Effort | Priority | Rationale |
|---------|-------|--------|----------|-----------|
| **Rate Limiting + Caching** | ğŸŸ¢ High | ğŸŸ¢ Low (1-2h) | **P0** | Blocks everything else; fixes current pain |
| **Market Context Widget** | ğŸŸ¢ High | ğŸŸ¡ Med (3-4h) | **P1** | High impact, moderate complexity |
| **VWAP Analysis** | ğŸŸ¡ Med | ğŸŸ¢ Low (2h) | **P1** | Quick win, valuable signal |
| **News Sentiment Scoring** | ğŸŸ¢ High | ğŸŸ¡ Med (4-5h) | **P2** | High value but needs AI prompt tuning |
| **Catalyst Categorization** | ğŸŸ¡ Med | ğŸŸ¢ Low (2h) | **P2** | Easy with Claude, good ROI |
| **Sector Heatmap** | ğŸŸ¡ Med | ğŸ”´ High (6-8h) | **P3** | Nice-to-have, significant UI work |
| **News Clustering** | ğŸ”´ Low | ğŸŸ¡ Med (3-4h) | **P4** | Lower priority, complex algorithm |
| **Volume Profile Chart** | ğŸ”´ Low | ğŸ”´ High (8-10h) | **P5** | Deferred to Phase 4 |

### Recommended Implementation Order

#### **Wave 1: Infrastructure (P0)** - 1-2 hours
Enable everything else to work smoothly
- Rate limiting decorator (5 stocks/sec for yfinance)
- Redis/Django cache for stock data (5-min TTL)
- Retry logic with exponential backoff

#### **Wave 2: Quick Wins (P1)** - 5-6 hours
High value, low-to-medium effort
- Market Context Widget (SPY, QQQ, VIX, futures)
- VWAP Analysis (compare current price to VWAP)
- Display improvements (last updated timestamp, cache indicators)

#### **Wave 3: AI Intelligence (P2)** - 6-7 hours
Leverage Claude to enhance news analysis
- News sentiment scoring (-1 to +1 scale)
- Catalyst categorization (earnings, FDA, M&A, insider, macro)
- Confidence indicators for AI analysis

#### **Wave 4: Polish (P3)** - 6-8 hours *(Optional for Phase 3)*
Nice-to-haves if time permits
- Sector heatmap visualization
- Economic calendar integration
- Pre-market vs regular hours volume comparison

---

## Technical Architecture

### Current Stack
```
Frontend: Django Templates + Tailwind CSS + Vanilla JS
Backend: Django 5.0 + Python 3.x
Data Sources: YFinance (stock data), Finnhub (news), FMP (unused)
AI: Anthropic Claude Sonnet 4.5
Cache: Django file-based cache (will upgrade to Redis)
Database: SQLite (dev), PostgreSQL (production)
```

### Proposed Additions

#### 1. Rate Limiting Layer
```python
# strategies/rate_limiter.py
from functools import wraps
from time import sleep, time
from threading import Lock

class RateLimiter:
    """Thread-safe rate limiter using token bucket algorithm"""

    def __init__(self, calls_per_second=5):
        self.rate = calls_per_second
        self.tokens = calls_per_second
        self.last_update = time()
        self.lock = Lock()

    def __call__(self, func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            with self.lock:
                now = time()
                elapsed = now - self.last_update
                self.tokens = min(self.rate, self.tokens + elapsed * self.rate)
                self.last_update = now

                if self.tokens < 1:
                    sleep((1 - self.tokens) / self.rate)
                    self.tokens = 0
                else:
                    self.tokens -= 1

            return func(*args, **kwargs)
        return wrapper

# Usage:
yfinance_limiter = RateLimiter(calls_per_second=5)

@yfinance_limiter
def get_stock_data(symbol):
    return yf.Ticker(symbol).info
```

**Why Token Bucket?**
- More flexible than fixed-window rate limiting
- Allows burst requests up to limit
- Thread-safe for concurrent scans
- Standard industry pattern

#### 2. Caching Layer
```python
# strategies/cache_utils.py
from django.core.cache import cache
from functools import wraps
import hashlib
import json

def cached(ttl_seconds=300, key_prefix=''):
    """
    Decorator for caching function results

    Args:
        ttl_seconds: Time to live in seconds (default 5 minutes)
        key_prefix: Prefix for cache key (e.g., 'stock_data')
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # Generate cache key from function name + args
            key_parts = [key_prefix, func.__name__] + [str(arg) for arg in args]
            key_str = ':'.join(key_parts)
            cache_key = hashlib.md5(key_str.encode()).hexdigest()

            # Try cache first
            cached_value = cache.get(cache_key)
            if cached_value is not None:
                return cached_value

            # Cache miss - call function
            result = func(*args, **kwargs)

            # Store in cache
            cache.set(cache_key, result, ttl_seconds)

            return result
        return wrapper
    return decorator

# Usage:
@cached(ttl_seconds=300, key_prefix='market_data')
def get_index_data(symbol):
    return yf.Ticker(symbol).info
```

**Cache Strategy:**
- Stock quotes: 5-minute TTL (balance freshness vs API load)
- News: 15-minute TTL (news doesn't change rapidly)
- Market indices: 1-minute TTL (more critical, faster refresh)
- User sessions: Use existing Django session cache

**Redis vs Django File Cache:**
- **Dev:** Django file cache (no extra setup)
- **Prod:** Redis (faster, better for concurrent access)
- Implementation: Abstract behind Django cache interface (easy swap)

#### 3. New Data Services

##### Market Context Service
```python
# strategies/market_context.py
from dataclasses import dataclass
from typing import Dict, Optional
import yfinance as yf
from .cache_utils import cached

@dataclass
class MarketContext:
    """Snapshot of overall market conditions"""
    spy_change: float          # S&P 500 change %
    qqq_change: float          # Nasdaq 100 change %
    vix_level: float           # VIX (fear index)
    es_futures: float          # E-mini S&P futures change %
    nq_futures: float          # Nasdaq futures change %
    market_sentiment: str      # 'bullish', 'neutral', 'bearish'
    last_updated: datetime

    @property
    def is_risk_on(self) -> bool:
        """Risk-on = SPY up, VIX down"""
        return self.spy_change > 0 and self.vix_level < 20

    @property
    def is_risk_off(self) -> bool:
        """Risk-off = SPY down, VIX elevated"""
        return self.spy_change < -0.5 and self.vix_level > 25

@cached(ttl_seconds=60, key_prefix='market_context')
def get_market_context() -> MarketContext:
    """Fetch current market conditions (cached 1 min)"""
    spy = yf.Ticker('SPY').info
    qqq = yf.Ticker('QQQ').info
    vix = yf.Ticker('^VIX').info
    es = yf.Ticker('ES=F').info  # E-mini S&P futures
    nq = yf.Ticker('NQ=F').info  # Nasdaq futures

    spy_change = ((spy['regularMarketPrice'] - spy['previousClose']) / spy['previousClose']) * 100
    qqq_change = ((qqq['regularMarketPrice'] - qqq['previousClose']) / qqq['previousClose']) * 100

    # Determine sentiment
    if spy_change > 0.5 and vix['regularMarketPrice'] < 18:
        sentiment = 'bullish'
    elif spy_change < -0.5 or vix['regularMarketPrice'] > 25:
        sentiment = 'bearish'
    else:
        sentiment = 'neutral'

    return MarketContext(
        spy_change=round(spy_change, 2),
        qqq_change=round(qqq_change, 2),
        vix_level=round(vix['regularMarketPrice'], 2),
        es_futures=round(((es['regularMarketPrice'] - es['previousClose']) / es['previousClose']) * 100, 2),
        nq_futures=round(((nq['regularMarketPrice'] - nq['previousClose']) / nq['previousClose']) * 100, 2),
        market_sentiment=sentiment,
        last_updated=datetime.now()
    )
```

##### VWAP Service
```python
# strategies/vwap_service.py
import yfinance as yf
import pandas as pd
from .cache_utils import cached

@cached(ttl_seconds=300, key_prefix='vwap')
def calculate_vwap(symbol: str) -> dict:
    """
    Calculate VWAP (Volume Weighted Average Price) for today

    Returns:
        Dict with vwap, current_price, distance_from_vwap_percent
    """
    ticker = yf.Ticker(symbol)

    # Get intraday data (1-min intervals)
    df = ticker.history(period='1d', interval='1m')

    if df.empty:
        return None

    # VWAP = Cumulative(Price * Volume) / Cumulative(Volume)
    df['cumulative_tpv'] = (df['Close'] * df['Volume']).cumsum()
    df['cumulative_volume'] = df['Volume'].cumsum()
    df['vwap'] = df['cumulative_tpv'] / df['cumulative_volume']

    vwap = df['vwap'].iloc[-1]
    current_price = df['Close'].iloc[-1]
    distance_pct = ((current_price - vwap) / vwap) * 100

    return {
        'symbol': symbol,
        'vwap': round(vwap, 2),
        'current_price': round(current_price, 2),
        'distance_from_vwap_percent': round(distance_pct, 2),
        'above_vwap': distance_pct > 0
    }
```

##### News Intelligence Service
```python
# strategies/news_intelligence.py
from anthropic import Anthropic
import json
from .finnhub_service import FinnhubClient

def analyze_news_sentiment(symbol: str, headlines: list) -> dict:
    """
    Use Claude to analyze news sentiment and categorize catalysts

    Args:
        symbol: Stock ticker
        headlines: List of news headline strings

    Returns:
        Dict with sentiment_score, catalyst_type, confidence, summary
    """
    client = Anthropic(api_key=settings.ANTHROPIC_API_KEY)

    headlines_text = '\n'.join([f"- {h}" for h in headlines[:10]])

    prompt = f"""Analyze these news headlines for {symbol} and provide:

Headlines:
{headlines_text}

Return JSON with:
1. sentiment_score: -1.0 (very bearish) to +1.0 (very bullish)
2. catalyst_type: earnings|fda|merger|partnership|insider|legal|macro|product|other
3. confidence: low|medium|high (how clear is the catalyst?)
4. one_sentence_summary: Brief explanation (max 15 words)

Example output:
{{"sentiment_score": 0.8, "catalyst_type": "earnings", "confidence": "high", "one_sentence_summary": "Strong Q4 earnings beat with raised guidance"}}

Respond with ONLY the JSON, no other text."""

    response = client.messages.create(
        model="claude-3-haiku-20240307",  # Cheaper model for this task
        max_tokens=200,
        messages=[{"role": "user", "content": prompt}]
    )

    # Parse JSON response
    try:
        result = json.loads(response.content[0].text)
        return result
    except json.JSONDecodeError:
        return {
            'sentiment_score': 0.0,
            'catalyst_type': 'other',
            'confidence': 'low',
            'one_sentence_summary': 'Unable to analyze news'
        }
```

**Why Haiku instead of Sonnet for sentiment?**
- 5x cheaper ($0.25/MTok vs $3/MTok input)
- Faster responses (200-500ms vs 2-5s)
- Sufficient capability for structured extraction
- Estimated cost: ~$0.0003 per stock (vs $0.002 for full research)

---

## Feature Implementation Plans

### Wave 1: Infrastructure (P0) - 1-2 hours

#### Feature 1.1: Rate Limiting Decorator
**Goal:** Prevent "Too Many Requests" errors from YFinance

**Implementation Steps:**
1. Create `strategies/rate_limiter.py` with token bucket algorithm
2. Apply `@yfinance_limiter` decorator to `get_stock_data()` in `stock_data.py`
3. Add logging to track throttling events
4. Add unit tests for rate limiter

**Files to Change:**
- `strategies/rate_limiter.py` (NEW - 80 lines)
- `strategies/stock_data.py` (lines 95-130, add decorator)
- `tests/test_rate_limiter.py` (NEW - 50 lines)

**Success Criteria:**
- âœ… No "Too Many Requests" errors during 400+ stock scans
- âœ… Scan completes in <45 seconds (was 30s, acceptable tradeoff for reliability)
- âœ… Logging shows throttling is working

**Risks:**
- ğŸŸ¡ May slow scans by 15-20% - ACCEPTED (reliability > speed)

#### Feature 1.2: Caching Layer
**Goal:** Reduce redundant API calls, speed up repeated scans

**Implementation Steps:**
1. Create `strategies/cache_utils.py` with `@cached` decorator
2. Install Redis (dev: `brew install redis`, prod: Redis Cloud)
3. Update `settings.py` to use Redis cache backend
4. Apply caching to:
   - `get_stock_data()` (5-min TTL)
   - `get_company_news()` (15-min TTL)
   - Market context functions (1-min TTL)
5. Add cache statistics to admin panel

**Files to Change:**
- `strategies/cache_utils.py` (NEW - 60 lines)
- `config/settings.py` (lines 180-195, cache config)
- `strategies/stock_data.py` (add `@cached` decorator)
- `strategies/finnhub_service.py` (add `@cached` decorator)

**Environment Changes:**
```bash
# .env additions
REDIS_URL=redis://localhost:6379/0  # Dev
# REDIS_URL=rediss://user:pass@host:port  # Prod
CACHE_DEFAULT_TIMEOUT=300  # 5 minutes
```

**Dependencies:**
```bash
pip install redis django-redis
```

**Success Criteria:**
- âœ… Second scan of same filters completes in <3 seconds (90% cache hit)
- âœ… Cache hit rate >70% during normal usage
- âœ… UI shows "cached" indicator with timestamp

**Risks:**
- ğŸŸ¢ Low risk - Django cache abstraction makes Redis optional
- ğŸŸ¡ Stale data risk - mitigated by short TTLs (5 min max)

#### Feature 1.3: Retry Logic
**Goal:** Handle transient API failures gracefully

**Implementation Steps:**
1. Add `tenacity` library for retry logic
2. Update `stock_data.py` to retry failed requests (3 attempts, exponential backoff)
3. Add logging for retry attempts
4. Update error handling to distinguish permanent vs transient failures

**Files to Change:**
- `strategies/stock_data.py` (lines 95-130, add retry decorator)
- `requirements.txt` (add `tenacity==8.2.3`)

**Code Example:**
```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10),
    reraise=True
)
def get_stock_data_with_retry(symbol):
    return yf.Ticker(symbol).info
```

**Success Criteria:**
- âœ… Transient failures auto-recover without user intervention
- âœ… Logging shows retry attempts
- âœ… Permanent failures (404, invalid symbol) don't retry

---

### Wave 2: Quick Wins (P1) - 5-6 hours

#### Feature 2.1: Market Context Widget
**Goal:** Show market overview at top of Pre-Market Movers page

**UI Mockup (Tailwind):**
```html
<!-- Top of page, above AI Research toggle -->
<div class="bg-white dark:bg-gray-800 rounded-lg shadow p-4 mb-6">
  <h3 class="text-sm font-semibold mb-3">Market Context</h3>

  <div class="grid grid-cols-5 gap-4 text-sm">
    <!-- SPY -->
    <div class="text-center">
      <div class="text-gray-500 dark:text-gray-400 text-xs">SPY</div>
      <div class="text-lg font-semibold text-green-600">+0.85%</div>
    </div>

    <!-- QQQ -->
    <div class="text-center">
      <div class="text-gray-500 dark:text-gray-400 text-xs">QQQ</div>
      <div class="text-lg font-semibold text-green-600">+1.23%</div>
    </div>

    <!-- VIX -->
    <div class="text-center">
      <div class="text-gray-500 dark:text-gray-400 text-xs">VIX</div>
      <div class="text-lg font-semibold">15.34</div>
    </div>

    <!-- Futures -->
    <div class="text-center">
      <div class="text-gray-500 dark:text-gray-400 text-xs">ES Futures</div>
      <div class="text-lg font-semibold text-green-600">+0.42%</div>
    </div>

    <!-- Sentiment -->
    <div class="text-center">
      <div class="text-gray-500 dark:text-gray-400 text-xs">Sentiment</div>
      <div class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-green-100 text-green-800">
        âœ… BULLISH
      </div>
    </div>
  </div>

  <div class="mt-2 text-xs text-gray-500 dark:text-gray-400 text-right">
    Updated: 9:28 AM ET â€¢ <span class="text-green-600">â—</span> Live
  </div>
</div>
```

**Implementation Steps:**
1. Create `strategies/market_context.py` service (code above)
2. Add view function `get_market_context_view()` in `strategies/views.py`
3. Update `pre_market_movers` view to fetch market context
4. Add widget to `pre_market_movers.html` template
5. Add auto-refresh via HTMX or simple JS (every 60 seconds)

**Files to Change:**
- `strategies/market_context.py` (NEW - 120 lines)
- `strategies/views.py` (add 30 lines)
- `strategies/templates/strategies/pre_market_movers.html` (add widget at line 16)
- `strategies/urls.py` (add route for `/market-context/` endpoint)

**Success Criteria:**
- âœ… Widget loads in <2 seconds
- âœ… Auto-refreshes every 60 seconds
- âœ… Clear visual indicators (color-coded)
- âœ… Works in dark mode

**Risks:**
- ğŸŸ¡ YFinance futures data may be unreliable (fallback to Finnhub if needed)
- ğŸŸ¢ Caching reduces risk of rate limiting

#### Feature 2.2: VWAP Analysis
**Goal:** Show VWAP comparison for tracked stocks

**UI Addition (in tracked movers section):**
```html
<!-- Add to tracked stock card -->
<div class="mt-2 text-xs text-gray-600 dark:text-gray-400">
  <span class="font-medium">VWAP:</span> $45.23
  <span class="text-green-600">(+2.3% above)</span>
</div>
```

**Implementation Steps:**
1. Create `strategies/vwap_service.py` (code above)
2. Add VWAP calculation to tracked movers view
3. Update template to display VWAP data
4. Add tooltip explaining VWAP significance

**Files to Change:**
- `strategies/vwap_service.py` (NEW - 80 lines)
- `strategies/views.py` (add VWAP fetch in tracked movers loop)
- `strategies/templates/strategies/pre_market_movers.html` (add VWAP display)

**Success Criteria:**
- âœ… VWAP calculated correctly (validated against TradingView)
- âœ… Displays for all tracked stocks
- âœ… Color-coded (green above, red below)

**Risks:**
- ğŸŸ¡ YFinance intraday data may be missing pre-market (show "N/A" if unavailable)
- ğŸŸ¢ Low complexity, standard calculation

---

### Wave 3: AI Intelligence (P2) - 6-7 hours

#### Feature 3.1: News Sentiment Scoring
**Goal:** Automatically score news sentiment (-1 to +1) using Claude Haiku

**UI Display:**
```html
<!-- In research results or tracked stock card -->
<div class="flex items-center space-x-2 mt-2">
  <div class="flex items-center space-x-1">
    <span class="text-xs text-gray-500">Sentiment:</span>
    <div class="w-16 h-2 bg-gray-200 rounded-full overflow-hidden">
      <div class="h-full bg-green-500" style="width: 80%"></div>
    </div>
    <span class="text-xs font-medium text-green-600">+0.8</span>
  </div>
  <span class="text-xs text-gray-400">(high confidence)</span>
</div>
```

**Implementation Steps:**
1. Create `strategies/news_intelligence.py` service
2. Update research workflow to include sentiment analysis
3. Add sentiment display to tracked movers
4. Store sentiment scores in session/database
5. Add cost tracking for sentiment API calls

**Files to Change:**
- `strategies/news_intelligence.py` (NEW - 150 lines)
- `strategies/views.py` (integrate sentiment into research flow)
- `strategies/templates/strategies/pre_market_movers.html` (add sentiment UI)
- `research/models.py` (add sentiment fields to ResearchSession model)

**Database Migration:**
```python
# research/migrations/000X_add_sentiment.py
class Migration(migrations.Migration):
    operations = [
        migrations.AddField(
            model_name='researchsession',
            name='sentiment_score',
            field=models.FloatField(null=True, blank=True),
        ),
        migrations.AddField(
            model_name='researchsession',
            name='catalyst_type',
            field=models.CharField(max_length=50, blank=True),
        ),
        migrations.AddField(
            model_name='researchsession',
            name='sentiment_confidence',
            field=models.CharField(max_length=20, blank=True),
        ),
    ]
```

**Prompt Engineering Notes:**
- Use Claude Haiku (5x cheaper than Sonnet)
- Strict JSON output format
- Include few-shot examples in prompt for consistency
- Consider sentiment calibration (test against manual labels)

**Success Criteria:**
- âœ… Sentiment scores match human judgment (>80% agreement on 20-sample test)
- âœ… Cost per sentiment analysis <$0.0005
- âœ… Response time <2 seconds
- âœ… Confidence levels accurately reflect clarity of news

**Risks:**
- ğŸŸ¡ Model may struggle with ambiguous news (multiple catalysts) - mitigated by confidence score
- ğŸŸ¡ Cost could add up if applied to all scanned stocks - ONLY apply to tracked stocks

#### Feature 3.2: Catalyst Categorization
**Goal:** Automatically tag news with catalyst type (earnings, FDA, M&A, etc.)

**UI Display:**
```html
<!-- Catalyst tag badge -->
<span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-purple-100 text-purple-800 dark:bg-purple-900 dark:text-purple-200">
  ğŸ“Š EARNINGS
</span>

<!-- Or for FDA approval -->
<span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-blue-100 text-blue-800">
  ğŸ’Š FDA APPROVAL
</span>
```

**Catalyst Categories:**
- **earnings** - Quarterly/annual results, guidance
- **fda** - Drug approvals, clinical trial results
- **merger** - M&A, acquisitions, spinoffs
- **partnership** - Strategic partnerships, deals
- **insider** - Insider buying/selling, executive changes
- **legal** - Lawsuits, regulatory issues
- **macro** - Economic data, sector trends
- **product** - New product launches
- **other** - Uncategorized

**Implementation:**
- Combined with sentiment analysis (same API call)
- Uses Claude Haiku with structured output
- Stored in database for filtering/sorting later

**Files to Change:**
- `strategies/news_intelligence.py` (already included in Feature 3.1)
- `strategies/templates/strategies/pre_market_movers.html` (add catalyst badges)
- `research/models.py` (catalyst_type field added in 3.1 migration)

**Success Criteria:**
- âœ… Catalyst categories accurate (>85% precision on test set)
- âœ… Visual badges clear and color-coded
- âœ… Hover tooltips explain each catalyst type

---

### Wave 4: Polish (P3) - Optional for Phase 3

#### Feature 4.1: Sector Heatmap *(DEFERRED)*
**Rationale:** High effort (6-8 hours), medium value
**Defer to Phase 4** - Focus on higher ROI features first

#### Feature 4.2: Economic Calendar *(DEFERRED)*
**Rationale:** Requires external API (Finnhub economic calendar or Alpha Vantage)
**Defer to Phase 4** - Evaluate user demand first

---

## Database Schema Changes

### Migration 1: Add Sentiment Fields to ResearchSession
```python
# research/migrations/000X_add_sentiment_fields.py

from django.db import migrations, models

class Migration(migrations.Migration):
    dependencies = [
        ('research', '000Y_previous_migration'),
    ]

    operations = [
        migrations.AddField(
            model_name='researchsession',
            name='sentiment_score',
            field=models.FloatField(
                null=True,
                blank=True,
                help_text='News sentiment from -1.0 (bearish) to +1.0 (bullish)'
            ),
        ),
        migrations.AddField(
            model_name='researchsession',
            name='catalyst_type',
            field=models.CharField(
                max_length=50,
                blank=True,
                choices=[
                    ('earnings', 'Earnings'),
                    ('fda', 'FDA/Clinical'),
                    ('merger', 'M&A'),
                    ('partnership', 'Partnership'),
                    ('insider', 'Insider Activity'),
                    ('legal', 'Legal/Regulatory'),
                    ('macro', 'Macro/Economic'),
                    ('product', 'Product Launch'),
                    ('other', 'Other'),
                ],
                help_text='Primary catalyst driving movement'
            ),
        ),
        migrations.AddField(
            model_name='researchsession',
            name='sentiment_confidence',
            field=models.CharField(
                max_length=20,
                blank=True,
                choices=[
                    ('low', 'Low'),
                    ('medium', 'Medium'),
                    ('high', 'High'),
                ],
                help_text='Confidence level in sentiment analysis'
            ),
        ),
        migrations.AddField(
            model_name='researchsession',
            name='vwap_distance',
            field=models.FloatField(
                null=True,
                blank=True,
                help_text='Distance from VWAP in percent'
            ),
        ),
    ]
```

### No Changes Needed For:
- `TrackedStock` model (existing fields sufficient)
- `ApiUsage` model (existing token tracking sufficient)

---

## API Requirements & Costs

### Current APIs
| Provider | Purpose | Cost | Rate Limits |
|----------|---------|------|-------------|
| YFinance | Stock data, quotes | Free | ~5 req/sec (unofficial) |
| Finnhub | News, market status | Free tier | 60 calls/min |
| Anthropic | AI research | $3/$15 per 1M tokens | 50 req/min |
| FMP | (Unused) | Free tier | 250 calls/day |

### New API Calls in Phase 3

#### Wave 1: Infrastructure
- **No new API calls** (reduces existing calls via caching)

#### Wave 2: Quick Wins
**Market Context Widget:**
- 5 additional YFinance calls per refresh (SPY, QQQ, VIX, ES, NQ)
- Refresh rate: 1/min
- Daily calls: 5 symbols Ã— 60 refreshes/hour Ã— 10 hours = **3,000 calls/day**
- **Cost: $0** (YFinance free)
- **Risk: ğŸŸ¢ Low** - Cached for 1 min, well within rate limits

**VWAP Analysis:**
- 1 additional YFinance call per tracked stock (intraday data)
- Average tracked stocks: 5-10 per session
- Daily calls: ~50-100
- **Cost: $0** (YFinance free)
- **Risk: ğŸŸ¢ Low** - Cached for 5 min

#### Wave 3: AI Intelligence
**News Sentiment Scoring:**
- Uses Claude Haiku (not Sonnet)
- 1 API call per tracked stock research
- Average tokens: 400 input + 100 output = 500 total
- Pricing: $0.25/MTok input + $1.25/MTok output
- **Cost per sentiment: $0.000225** (~$0.0002)
- Daily usage: 20 stocks researched/day
- **Daily cost: $0.005** (~half a cent)

**Cost Comparison:**
- Current full research (Sonnet): $0.002-0.003 per stock
- Adding sentiment (Haiku): +$0.0002 per stock (+7% cost increase)
- **Total Phase 3 cost per stock: $0.0022-0.0032**

### Monthly Cost Projections
**Current (Phase 2):**
- 20 stocks researched/day Ã— 30 days = 600 researches/month
- Cost: 600 Ã— $0.0025 = **$1.50/month**

**Phase 3 (with sentiment):**
- 600 researches/month
- Cost: 600 Ã— $0.0027 = **$1.62/month**
- **Increase: +$0.12/month** (8% increase)

**Conclusion:** Phase 3 adds minimal cost (~$0.10/month) while significantly increasing value.

---

## UI/UX Design

### Page Layout (Post-Phase 3)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Header (existing)                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Market Context Widget (NEW)                             â”‚
â”‚  SPY  â”‚  QQQ  â”‚  VIX  â”‚  Futures  â”‚  Sentiment         â”‚
â”‚ +0.85% â”‚ +1.2% â”‚ 15.3  â”‚  +0.42%   â”‚  âœ… BULLISH       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ AI Research Toggle (existing)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Discover Pre-Market Movers (existing)                   â”‚
â”‚  [Filters] [Scan Button]                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Scan Results (existing)                                 â”‚
â”‚  [Tracked stocks with NEW sentiment + VWAP data]        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Footer (existing)                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Tracked Stock Card Enhancements
```html
<!-- BEFORE (Phase 2) -->
<div class="stock-card">
  <div class="symbol">AAPL</div>
  <div class="price">$175.43 (+3.2%)</div>
  <div class="volume">RVOL: 4.5x</div>
  <button>Research This</button>
</div>

<!-- AFTER (Phase 3) -->
<div class="stock-card">
  <div class="header">
    <div class="symbol">AAPL</div>
    <span class="badge badge-earnings">ğŸ“Š EARNINGS</span>  <!-- NEW -->
  </div>

  <div class="price">$175.43 (+3.2% from close)</div>

  <div class="metrics">
    <div>RVOL: 4.5x</div>
    <div class="vwap">VWAP: $173.20 (+1.3% above)</div>  <!-- NEW -->
  </div>

  <!-- NEW Sentiment Bar -->
  <div class="sentiment">
    <span class="label">Sentiment:</span>
    <div class="sentiment-bar" style="width: 80%"></div>
    <span class="score">+0.8</span>
    <span class="confidence">(high)</span>
  </div>

  <div class="news">
    Strong Q1 earnings beat with iPhone growth...
  </div>

  <button>Research This</button>
</div>
```

### Color Coding System

**Sentiment Scores:**
- `+0.7 to +1.0` = Dark Green (`bg-green-600`)
- `+0.3 to +0.7` = Light Green (`bg-green-400`)
- `-0.3 to +0.3` = Gray (`bg-gray-400`)
- `-0.7 to -0.3` = Light Red (`bg-red-400`)
- `-1.0 to -0.7` = Dark Red (`bg-red-600`)

**Catalyst Types:**
- Earnings: Purple (`bg-purple-100`)
- FDA: Blue (`bg-blue-100`)
- M&A: Indigo (`bg-indigo-100`)
- Insider: Yellow (`bg-yellow-100`)
- Legal: Red (`bg-red-100`)
- Macro: Gray (`bg-gray-100`)
- Other: Gray (`bg-gray-50`)

**Market Sentiment:**
- Bullish: Green with âœ…
- Neutral: Gray with âšª
- Bearish: Red with âš ï¸

### Dark Mode Support
- All new components must support dark mode variants
- Test with `dark:` Tailwind classes
- Ensure sufficient contrast (WCAG AA compliance)

---

## Testing Strategy

### Unit Tests
**New Test Files:**
1. `tests/test_rate_limiter.py` - Rate limiting logic
2. `tests/test_cache_utils.py` - Caching decorator
3. `tests/test_market_context.py` - Market context calculations
4. `tests/test_vwap_service.py` - VWAP calculation accuracy
5. `tests/test_news_intelligence.py` - Sentiment analysis (mocked Claude)

**Coverage Goals:**
- Rate limiter: 100% (critical for reliability)
- Cache utils: 95%
- Market context: 85%
- VWAP: 90% (validate against known values)
- News intelligence: 80% (mock API calls)

### Integration Tests
**Test Scenarios:**
1. Full scan with rate limiting + caching enabled
2. Market context widget loads in <2s
3. VWAP displays correctly for tracked stocks
4. Sentiment analysis returns valid JSON
5. Catalyst categorization assigns correct tags

### Manual Testing Checklist
**Pre-Market Testing (8:00-9:30 AM ET):**
- [ ] Market context widget shows pre-market futures data
- [ ] VWAP calculation uses pre-market prices
- [ ] Sentiment analysis reflects overnight news
- [ ] No rate limiting errors during peak pre-market scan

**User Workflow Testing:**
- [ ] Scan 100+ stocks with new infrastructure
- [ ] Track 10 stocks and verify all Phase 3 features display
- [ ] Research 5 stocks and confirm sentiment scores are reasonable
- [ ] Check token dashboard for cost accuracy

**Performance Testing:**
- [ ] First scan (cold cache): <45s for 405 stocks
- [ ] Second scan (warm cache): <5s
- [ ] Market context refresh: <2s
- [ ] Sentiment analysis per stock: <3s

### A/B Testing (Post-Launch)
**Metrics to Compare:**
- Time from scan to track decision (with vs without sentiment)
- Research conversion rate (tracked stocks that get researched)
- User satisfaction survey (1-5 scale)

---

## Risk Assessment

### High Risk (ğŸ”´)
*None identified* - Architecture is conservative and builds on proven patterns

### Medium Risk (ğŸŸ¡)
1. **YFinance Rate Limiting**
   - *Risk:* Even with rate limiter, YFinance may block aggressive usage
   - *Mitigation:* Implement exponential backoff, consider FMP API as backup
   - *Severity:* Medium - affects scan reliability but not critical features

2. **Claude Sentiment Accuracy**
   - *Risk:* Sentiment scores may not match user expectations
   - *Mitigation:* Use Haiku with well-tested prompts, show confidence levels
   - *Severity:* Low-Medium - wrong sentiment is annoying but not breaking

3. **Redis Dependency (Production)**
   - *Risk:* Redis outage breaks caching
   - *Mitigation:* Graceful fallback to direct API calls, use Redis Cloud for reliability
   - *Severity:* Low - system degrades but doesn't break

### Low Risk (ğŸŸ¢)
1. **VWAP Calculation Accuracy**
   - *Risk:* VWAP formula implementation bugs
   - *Mitigation:* Validate against TradingView, add unit tests
   - *Severity:* Low - cosmetic feature, doesn't affect core workflow

2. **Market Context Widget Performance**
   - *Risk:* Widget slows page load
   - *Mitigation:* Aggressive caching (1-min TTL), async loading
   - *Severity:* Low - non-blocking, can be lazy-loaded

---

## Timeline & Effort Estimates

### Development Timeline
```
Week 1: Wave 1 (Infrastructure)
â”œâ”€ Day 1-2: Rate limiting + caching (2 hours dev + 2 hours testing)
â”œâ”€ Day 3: Retry logic (1 hour dev + 1 hour testing)
â””â”€ Day 4-5: Integration testing, bug fixes, documentation

Week 2: Wave 2 (Quick Wins)
â”œâ”€ Day 1-2: Market context widget (4 hours)
â”œâ”€ Day 3: VWAP analysis (2 hours)
â””â”€ Day 4-5: Integration, UI polish, testing

Week 3: Wave 3 (AI Intelligence)
â”œâ”€ Day 1-2: News sentiment service (4 hours)
â”œâ”€ Day 3: Catalyst categorization (2 hours, combined with sentiment)
â”œâ”€ Day 4: Database migrations, UI integration (2 hours)
â””â”€ Day 5: Testing, prompt tuning, cost validation

Week 4: Polish & Testing
â”œâ”€ Day 1-2: Bug fixes, edge cases
â”œâ”€ Day 3: Performance optimization
â”œâ”€ Day 4: Documentation updates
â””â”€ Day 5: User acceptance testing, deploy to production
```

### Total Effort Estimate
- **Development:** 18-22 hours
- **Testing:** 8-10 hours
- **Documentation:** 4-5 hours
- **Total:** **30-37 hours** (~1 week full-time, 2-3 weeks part-time)

### Dependencies
- Redis installation (5 min setup)
- `tenacity` library install (1 min)
- Database migration (2 min)

---

## Success Metrics

### Performance Metrics
| Metric | Current (Phase 2) | Target (Phase 3) |
|--------|-------------------|------------------|
| Scan time (405 stocks, cold cache) | 30-40s | <45s |
| Scan time (warm cache) | N/A | <5s |
| Rate limit errors per scan | 10-30 | 0 |
| Cache hit rate | 0% | >70% |
| Time to research decision | ~3 min | <1 min |

### Cost Metrics
| Metric | Current | Target |
|--------|---------|--------|
| Cost per stock research | $0.002-0.003 | $0.0022-0.0032 |
| Monthly API costs | $1.50 | <$2.00 |
| Token usage per session | 15K | 18K |

### User Experience Metrics
| Metric | Target |
|--------|--------|
| Market context load time | <2s |
| Sentiment accuracy vs manual labels | >80% |
| User satisfaction (1-5) | >4.0 |
| Feature usage rate (% sessions using Phase 3 features) | >60% |

### Adoption Metrics (Post-Launch)
- **Week 1:** 30% of users notice new features
- **Week 2:** 50% actively use market context widget
- **Week 4:** 70% rely on sentiment scores for decisions
- **Week 8:** Feature considered "essential" by users

---

## Questions for Codex Review

### Architecture & Design
1. **Caching Strategy:** Is 5-min TTL for stock data appropriate? Should it be dynamic based on market hours (1-min during market hours, 10-min after close)?

2. **Rate Limiting:** Token bucket vs fixed window - is token bucket overkill for this use case?

3. **Market Context Widget:** Should futures data (ES/NQ) be included, or is that too advanced for initial implementation? Finnhub doesn't provide futures reliably.

4. **VWAP Calculation:** Should we calculate VWAP from market open or from pre-market open (4:00 AM ET)? Which is more useful for pre-market movers?

### AI & Sentiment Analysis
5. **Model Choice:** Is Haiku sufficient for sentiment analysis, or should we use Sonnet for better accuracy despite higher cost?

6. **Prompt Engineering:** Should sentiment prompts include few-shot examples, or is zero-shot sufficient with clear instructions?

7. **Sentiment Scale:** Is -1 to +1 the right scale, or should we use categorical (very bearish, bearish, neutral, bullish, very bullish)?

8. **Confidence Levels:** How do we define "low" vs "medium" vs "high" confidence objectively? Should Claude self-assess or use heuristics (e.g., multiple conflicting headlines = low confidence)?

### Performance & Cost
9. **Redis Requirement:** Should Redis be mandatory for Phase 3, or keep Django file-based cache as fallback? What's the performance difference?

10. **Sentiment Cost Control:** Should sentiment analysis be:
    - Applied to ALL tracked stocks automatically (current plan)
    - Only on-demand when user clicks "Analyze Sentiment"
    - Only for stocks with RVOL >5x (high conviction movers)

11. **YFinance Backup:** If YFinance rate limits persist, should we switch to FMP API (250 calls/day limit on free tier)? Is it worth the complexity?

### UX & Implementation
12. **Market Context Refresh:** Auto-refresh every 60s or manual refresh button? Balance between freshness and unnecessary API calls.

13. **Sentiment Display:** Should sentiment be shown as:
    - Current plan: progress bar + numeric score
    - Alternative: emoji scale (ğŸ˜¢ ğŸ˜Ÿ ğŸ˜ ğŸ™‚ ğŸ˜Š)
    - Alternative: colored dots (red/yellow/green)

14. **Catalyst Badges:** Should catalyst types be clickable filters (e.g., "show only earnings movers")? Adds complexity but high value.

### Testing & Rollout
15. **Feature Flags:** Should Phase 3 features be behind feature flags for gradual rollout, or ship all-at-once?

16. **Sentiment Validation:** How do we validate sentiment accuracy without manual labeling? Compare to FinBERT or other financial sentiment models?

17. **A/B Testing:** Is it worth A/B testing sentiment feature (50% with, 50% without) to measure impact on user behavior?

### Phase 4 Planning
18. **Priority Adjustment:** Based on this plan, would you re-prioritize any features? E.g., defer VWAP in favor of sector heatmap?

19. **User Feedback:** Should we ship Wave 1+2 first, gather feedback, then proceed with Wave 3 (AI features)?

20. **Technical Debt:** Are there any concerns about technical debt or architectural decisions that will be hard to change later?

---

## Appendix A: API Endpoint Changes

### New Endpoints
```python
# strategies/urls.py additions

urlpatterns = [
    # ... existing routes ...

    # Market context (AJAX endpoint for widget refresh)
    path('market-context/', views.market_context_view, name='market_context'),

    # VWAP data for tracked stocks
    path('vwap/<str:symbol>/', views.vwap_view, name='vwap'),

    # Sentiment analysis (called during research)
    path('analyze-sentiment/<str:symbol>/', views.analyze_sentiment_view, name='analyze_sentiment'),
]
```

---

## Appendix B: Configuration Changes

### Environment Variables (.env)
```bash
# ... existing vars ...

# Phase 3: Caching
REDIS_URL=redis://localhost:6379/0
CACHE_DEFAULT_TIMEOUT=300

# Phase 3: Rate Limiting
YFINANCE_RATE_LIMIT=5  # calls per second
FINNHUB_RATE_LIMIT=60  # calls per minute

# Phase 3: AI Sentiment
SENTIMENT_MODEL=claude-3-haiku-20240307
SENTIMENT_ENABLED=True
```

### Django Settings (config/settings.py)
```python
# Cache configuration
if os.getenv('REDIS_URL'):
    CACHES = {
        'default': {
            'BACKEND': 'django_redis.cache.RedisCache',
            'LOCATION': os.getenv('REDIS_URL'),
            'OPTIONS': {
                'CLIENT_CLASS': 'django_redis.client.DefaultClient',
            },
            'KEY_PREFIX': 'picker',
            'TIMEOUT': int(os.getenv('CACHE_DEFAULT_TIMEOUT', 300)),
        }
    }
else:
    # Fallback to file-based cache
    CACHES = {
        'default': {
            'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',
            'LOCATION': '/tmp/django_cache',
            'TIMEOUT': 300,
        }
    }

# Rate limiting
YFINANCE_RATE_LIMIT = int(os.getenv('YFINANCE_RATE_LIMIT', 5))
FINNHUB_RATE_LIMIT = int(os.getenv('FINNHUB_RATE_LIMIT', 60))

# Sentiment analysis
SENTIMENT_MODEL = os.getenv('SENTIMENT_MODEL', 'claude-3-haiku-20240307')
SENTIMENT_ENABLED = os.getenv('SENTIMENT_ENABLED', 'True').lower() == 'true'
```

---

## Appendix C: Dependencies

### New Python Packages
```
# requirements.txt additions
redis==5.0.1
django-redis==5.4.0
tenacity==8.2.3
```

### Installation
```bash
pip install redis django-redis tenacity
```

### Redis Installation
```bash
# macOS
brew install redis
brew services start redis

# Linux
sudo apt-get install redis-server
sudo systemctl start redis

# Verify
redis-cli ping  # Should return "PONG"
```

---

## Codex Review Response & Implementation Adjustments

**Review Date:** October 30, 2025
**Review Status:** âœ… Addressed

### Summary of Codex Feedback
Codex identified several operational risks and edge cases that need tightening before implementation. All concerns have been addressed below with updated implementation plans.

---

### 1. Rate Limiter Scope - Multi-Process Limitation âš ï¸

**Issue Identified:**
> Token bucket RateLimiter is thread-safe within one Python process, but won't coordinate across multiple runserver threads, Gunicorn workers, or horizontal scaling.

**Impact:** High - Production deployments use multiple workers

**Resolution:**

#### Development (Single Process)
Keep in-memory token bucket (sufficient for dev/testing):
```python
# strategies/rate_limiter.py (dev version)
class RateLimiter:
    """Thread-safe rate limiter for single-process dev environment"""
    # ... original implementation ...
```

#### Production (Multi-Process)
Use Redis-based rate limiter with shared state:
```python
# strategies/rate_limiter.py (production version)
from django.core.cache import cache
from time import time, sleep
from functools import wraps

class RedisRateLimiter:
    """
    Redis-backed rate limiter for multi-process/multi-worker environments

    Uses Redis for shared state across processes/machines.
    Implements sliding window counter algorithm.
    """

    def __init__(self, calls_per_second=5, window_seconds=1):
        self.calls_per_second = calls_per_second
        self.window_seconds = window_seconds

    def __call__(self, func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            key = f"rate_limit:{func.__name__}"
            now = time()

            # Sliding window: clear old entries, count recent calls
            cache_data = cache.get(key, {'calls': [], 'last_check': now})

            # Remove calls outside window
            cache_data['calls'] = [
                call_time for call_time in cache_data['calls']
                if now - call_time < self.window_seconds
            ]

            # Check if limit exceeded
            if len(cache_data['calls']) >= self.calls_per_second:
                oldest_call = min(cache_data['calls'])
                sleep_time = self.window_seconds - (now - oldest_call)
                if sleep_time > 0:
                    sleep(sleep_time)
                    now = time()
                    cache_data['calls'] = []  # Reset after sleep

            # Record this call
            cache_data['calls'].append(now)
            cache_data['last_check'] = now

            # Save to Redis (TTL = window_seconds to auto-cleanup)
            cache.set(key, cache_data, timeout=self.window_seconds * 2)

            return func(*args, **kwargs)
        return wrapper

# Usage:
if settings.DEBUG or not cache.get('_redis_test'):
    # Development: Use in-memory limiter
    yfinance_limiter = RateLimiter(calls_per_second=5)
else:
    # Production: Use Redis-backed limiter
    yfinance_limiter = RedisRateLimiter(calls_per_second=5)
```

**Configuration:**
```python
# config/settings.py
# Feature flag to choose rate limiter
USE_REDIS_RATE_LIMITER = not DEBUG  # Auto-detect based on environment
```

**Documentation Update:**
```markdown
### Rate Limiter Architecture

**Development:** In-memory token bucket (thread-safe, single process)
**Production:** Redis-backed sliding window (multi-process, horizontally scalable)

**Deployment Note:** Redis must be running in production for rate limiting to work across workers.
```

**Status:** âœ… Fixed - Added Redis-based rate limiter for production

---

### 2. Caching Decorator - Arg Hashing Stability ğŸ›

**Issue Identified:**
> hashing `str(arg)` breaks on dicts/objects (TypeError or unstable ordering)

**Impact:** Medium - Could cause cache key collisions or errors

**Resolution:**

**Updated Caching Decorator:**
```python
# strategies/cache_utils.py (FIXED VERSION)
from django.core.cache import cache
from functools import wraps
import hashlib
import json
import logging

logger = logging.getLogger(__name__)

def cached(ttl_seconds=300, key_prefix=''):
    """
    Decorator for caching function results with stable key generation.

    Handles dicts, lists, and complex objects safely.

    Args:
        ttl_seconds: Time to live in seconds (default 5 minutes)
        key_prefix: Prefix for cache key (e.g., 'stock_data')
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # Generate stable cache key
            try:
                # Normalize args for hashing
                normalized_args = []
                for arg in args:
                    if isinstance(arg, (dict, list)):
                        # Serialize dicts/lists to JSON with sorted keys
                        normalized_args.append(json.dumps(arg, sort_keys=True))
                    elif hasattr(arg, '__dict__'):
                        # Handle objects by serializing __dict__
                        normalized_args.append(json.dumps(arg.__dict__, sort_keys=True))
                    else:
                        # Primitives (str, int, float) are safe
                        normalized_args.append(str(arg))

                # Include kwargs in key (sorted for stability)
                if kwargs:
                    normalized_args.append(json.dumps(kwargs, sort_keys=True))

                # Build cache key
                key_parts = [key_prefix, func.__name__] + normalized_args
                key_str = ':'.join(key_parts)
                cache_key = hashlib.md5(key_str.encode()).hexdigest()[:32]  # Limit length

            except (TypeError, ValueError) as e:
                # If key generation fails, skip caching
                logger.warning(f"Cache key generation failed for {func.__name__}: {e}")
                return func(*args, **kwargs)

            # Try cache first
            cached_value = cache.get(cache_key)
            if cached_value is not None:
                logger.debug(f"Cache HIT: {func.__name__} ({cache_key[:8]}...)")
                return cached_value

            # Cache miss - call function
            logger.debug(f"Cache MISS: {func.__name__} ({cache_key[:8]}...)")
            result = func(*args, **kwargs)

            # Store in cache
            try:
                cache.set(cache_key, result, ttl_seconds)
            except Exception as e:
                logger.warning(f"Failed to cache result for {func.__name__}: {e}")

            return result

        # Add cache inspection methods
        wrapper.cache_info = lambda: {
            'function': func.__name__,
            'ttl': ttl_seconds,
            'prefix': key_prefix
        }

        return wrapper
    return decorator
```

**Test Cases:**
```python
# tests/test_cache_utils.py
def test_cache_with_dict_args():
    @cached(ttl_seconds=60, key_prefix='test')
    def func_with_dict(data):
        return data['value']

    # These should generate the same cache key (order doesn't matter)
    result1 = func_with_dict({'b': 2, 'a': 1, 'value': 42})
    result2 = func_with_dict({'a': 1, 'b': 2, 'value': 42})

    assert result1 == result2 == 42
    # Second call should be cached (verify via logging)
```

**Status:** âœ… Fixed - Added JSON serialization with sorted keys for stable hashing

---

### 3. YFinance Throughput Monitoring ğŸ“Š

**Issue Identified:**
> Plan assumes 5 calls/sec and 5-min TTL work, but should record actual call counts during testing and have backup plan if 429 responses occur.

**Impact:** High - Could break scans in production

**Resolution:**

#### API Call Monitoring
```python
# strategies/api_monitoring.py (NEW FILE)
from django.core.cache import cache
from datetime import datetime, timedelta
import logging

logger = logging.getLogger(__name__)

class ApiCallMonitor:
    """Track API call rates and detect rate limiting"""

    def __init__(self, api_name, rate_limit_threshold=0.1):
        self.api_name = api_name
        self.rate_limit_threshold = rate_limit_threshold  # 10% failure rate

    def record_call(self, success=True, response_code=None):
        """Record an API call outcome"""
        key = f"api_monitor:{self.api_name}:calls"

        # Get current window stats (last 5 minutes)
        stats = cache.get(key, {'total': 0, 'failed': 0, 'rate_limited': 0, 'start_time': datetime.now().isoformat()})

        stats['total'] += 1
        if not success:
            stats['failed'] += 1
        if response_code == 429:
            stats['rate_limited'] += 1

        cache.set(key, stats, timeout=300)  # 5-min window

        # Check if we're hitting rate limits
        if stats['total'] > 20:  # Minimum sample size
            rate_limited_pct = stats['rate_limited'] / stats['total']
            if rate_limited_pct > self.rate_limit_threshold:
                logger.warning(
                    f"âš ï¸ {self.api_name} rate limit threshold exceeded: "
                    f"{stats['rate_limited']}/{stats['total']} calls "
                    f"({rate_limited_pct:.1%}) resulted in 429"
                )
                # Trigger alert or fallback logic
                self._handle_rate_limit_exceeded(stats)

    def _handle_rate_limit_exceeded(self, stats):
        """Handle excessive rate limiting"""
        # Options:
        # 1. Increase cache TTL temporarily
        # 2. Switch to backup API (FMP, Alpha Vantage)
        # 3. Send alert to admin
        # 4. Reduce scan frequency

        # For now, just log
        logger.error(f"ğŸš¨ {self.api_name} rate limiting detected - consider switching APIs")

    def get_stats(self):
        """Get current API usage stats"""
        key = f"api_monitor:{self.api_name}:calls"
        return cache.get(key, {'total': 0, 'failed': 0, 'rate_limited': 0})

# Global monitors
yfinance_monitor = ApiCallMonitor('yfinance', rate_limit_threshold=0.05)
finnhub_monitor = ApiCallMonitor('finnhub', rate_limit_threshold=0.05)
```

#### Update stock_data.py to use monitoring:
```python
# strategies/stock_data.py
from .api_monitoring import yfinance_monitor

@yfinance_limiter
@cached(ttl_seconds=300, key_prefix='stock_data')
def get_stock_data(symbol):
    try:
        ticker = yf.Ticker(symbol)
        info = ticker.info
        yfinance_monitor.record_call(success=True)
        return info
    except requests.exceptions.HTTPError as e:
        response_code = e.response.status_code if hasattr(e, 'response') else None
        yfinance_monitor.record_call(success=False, response_code=response_code)

        if response_code == 429:
            logger.warning(f"Rate limited for {symbol} - increasing cache TTL")
            # Fallback: try cached data even if expired
            # Or switch to backup API
        raise
```

#### Backup API Configuration:
```python
# .env
YFINANCE_ENABLED=True
FMP_ENABLED=False  # Backup API
ALPHA_VANTAGE_ENABLED=False  # Backup API

# API priority order
API_FALLBACK_ORDER=yfinance,fmp,alpha_vantage
```

**Dashboard Integration:**
Add API health dashboard to admin:
```python
# strategies/views.py
def api_health_view(request):
    """Show API call statistics and health"""
    yf_stats = yfinance_monitor.get_stats()
    fh_stats = finnhub_monitor.get_stats()

    return render(request, 'strategies/api_health.html', {
        'yfinance': yf_stats,
        'finnhub': fh_stats,
    })
```

**Status:** âœ… Added - API call monitoring with alert system

---

### 4. Market Context Widget - Network Request Optimization âš¡

**Issue Identified:**
> Each market context request calls Ticker.info 5 times (SPY, QQQ, VIX, ES, NQ) - even cached at 60s this can be spiky. Consider batch endpoints or fast_info.

**Impact:** Medium - Could slow page loads

**Resolution:**

**Optimized Implementation using fast_info:**
```python
# strategies/market_context.py (OPTIMIZED VERSION)
import yfinance as yf
from dataclasses import dataclass
from datetime import datetime
from .cache_utils import cached
from .rate_limiter import yfinance_limiter
import logging

logger = logging.getLogger(__name__)

@dataclass
class MarketContext:
    """Snapshot of overall market conditions"""
    spy_change: float
    qqq_change: float
    vix_level: float
    es_futures: float  # May be None if unavailable
    nq_futures: float  # May be None if unavailable
    market_sentiment: str
    last_updated: datetime
    data_quality: str  # 'full', 'partial', 'degraded'

@yfinance_limiter
@cached(ttl_seconds=60, key_prefix='market_context')
def get_market_context() -> MarketContext:
    """
    Fetch current market conditions (cached 1 min)

    Uses fast_info instead of info for better performance.
    Gracefully degrades if some symbols fail.
    """
    symbols_data = {}
    failed_symbols = []

    # Fetch core symbols (SPY, QQQ, VIX are most critical)
    critical_symbols = ['SPY', 'QQQ', '^VIX']
    optional_symbols = ['ES=F', 'NQ=F']  # Futures can be unreliable

    # Fetch critical symbols
    for symbol in critical_symbols:
        try:
            ticker = yf.Ticker(symbol)
            # Use fast_info (faster, fewer fields)
            fast_info = ticker.fast_info
            symbols_data[symbol] = {
                'current': fast_info.last_price,
                'previous': fast_info.previous_close,
            }
        except Exception as e:
            logger.error(f"Failed to fetch {symbol}: {e}")
            failed_symbols.append(symbol)

    # Fetch optional symbols (don't fail if these are unavailable)
    for symbol in optional_symbols:
        try:
            ticker = yf.Ticker(symbol)
            fast_info = ticker.fast_info
            symbols_data[symbol] = {
                'current': fast_info.last_price,
                'previous': fast_info.previous_close,
            }
        except Exception as e:
            logger.debug(f"Futures data unavailable for {symbol}: {e}")
            symbols_data[symbol] = None

    # Calculate changes
    def calc_change(data):
        if data and data['current'] and data['previous']:
            return ((data['current'] - data['previous']) / data['previous']) * 100
        return None

    spy_change = calc_change(symbols_data.get('SPY'))
    qqq_change = calc_change(symbols_data.get('QQQ'))
    vix_level = symbols_data.get('^VIX', {}).get('current') if symbols_data.get('^VIX') else None

    # Futures (may be None)
    es_change = calc_change(symbols_data.get('ES=F'))
    nq_change = calc_change(symbols_data.get('NQ=F'))

    # Determine sentiment
    if spy_change is not None and vix_level is not None:
        if spy_change > 0.5 and vix_level < 18:
            sentiment = 'bullish'
        elif spy_change < -0.5 or vix_level > 25:
            sentiment = 'bearish'
        else:
            sentiment = 'neutral'
    else:
        sentiment = 'unknown'

    # Data quality indicator
    if len(failed_symbols) == 0:
        quality = 'full'
    elif len(failed_symbols) < 2:
        quality = 'partial'
    else:
        quality = 'degraded'

    return MarketContext(
        spy_change=round(spy_change, 2) if spy_change else 0.0,
        qqq_change=round(qqq_change, 2) if qqq_change else 0.0,
        vix_level=round(vix_level, 2) if vix_level else 0.0,
        es_futures=round(es_change, 2) if es_change else None,
        nq_futures=round(nq_change, 2) if nq_change else None,
        market_sentiment=sentiment,
        last_updated=datetime.now(),
        data_quality=quality
    )
```

**UI Update to show data quality:**
```html
<!-- Market context widget with quality indicator -->
<div class="market-context-widget">
  <!-- ... existing content ... -->

  <div class="text-xs text-gray-500">
    Updated: {{ context.last_updated|date:"g:i A" }} ET â€¢
    {% if context.data_quality == 'full' %}
      <span class="text-green-600">â— Live</span>
    {% elif context.data_quality == 'partial' %}
      <span class="text-yellow-600">â— Partial Data</span>
    {% else %}
      <span class="text-red-600">â— Degraded</span>
    {% endif %}
  </div>
</div>
```

**Performance Improvements:**
- `fast_info` is 2-3x faster than `info` (fewer API calls internally)
- Graceful degradation (show partial data if futures unavailable)
- Quality indicator informs user of data completeness

**Status:** âœ… Optimized - Using fast_info with graceful degradation

---

### 5. VWAP Calculation - Market Hours Fallback ğŸ•’

**Issue Identified:**
> ticker.history(period='1d', interval='1m') returns empty DataFrame when markets are closed. Plan for graceful fallbacks.

**Impact:** Medium - VWAP won't display outside market hours

**Resolution:**

**Updated VWAP Service with Fallbacks:**
```python
# strategies/vwap_service.py (FIXED VERSION)
import yfinance as yf
import pandas as pd
from datetime import datetime, time
from .cache_utils import cached
import logging

logger = logging.getLogger(__name__)

def is_market_hours():
    """Check if current time is during US market hours (9:30 AM - 4:00 PM ET)"""
    from zoneinfo import ZoneInfo  # Python 3.9+ standard library

    # Get current time in US Eastern timezone
    et_tz = ZoneInfo('America/New_York')
    now_et = datetime.now(et_tz)

    market_open = time(9, 30)
    market_close = time(16, 0)

    current_time = now_et.time()

    # Check if weekday (Mon-Fri)
    if now_et.weekday() >= 5:  # 5=Saturday, 6=Sunday
        return False

    return market_open <= current_time <= market_close

@cached(ttl_seconds=300, key_prefix='vwap')
def calculate_vwap(symbol: str) -> dict:
    """
    Calculate VWAP (Volume Weighted Average Price) for today.

    Falls back gracefully when market is closed or data unavailable.

    Returns:
        Dict with vwap, current_price, distance_from_vwap_percent
        Returns None if VWAP cannot be calculated
    """
    ticker = yf.Ticker(symbol)

    # Try intraday data (1-min intervals for today)
    try:
        df = ticker.history(period='1d', interval='1m')
    except Exception as e:
        logger.warning(f"Failed to fetch intraday data for {symbol}: {e}")
        return None

    # Check if we got data
    if df.empty:
        logger.debug(f"No intraday data for {symbol} - market may be closed")

        # Fallback 1: Try 5-minute intervals (less granular but more available)
        try:
            df = ticker.history(period='1d', interval='5m')
        except Exception as e:
            logger.warning(f"5-min fallback failed for {symbol}: {e}")

        if df.empty:
            # Fallback 2: Use previous trading day's VWAP
            try:
                df = ticker.history(period='5d', interval='1h')
                if not df.empty:
                    logger.info(f"Using previous session VWAP for {symbol}")
            except Exception:
                pass

    if df.empty:
        logger.warning(f"Cannot calculate VWAP for {symbol} - no data available")
        return None

    try:
        # VWAP = Cumulative(Price * Volume) / Cumulative(Volume)
        df['typical_price'] = (df['High'] + df['Low'] + df['Close']) / 3
        df['tpv'] = df['typical_price'] * df['Volume']
        df['cumulative_tpv'] = df['tpv'].cumsum()
        df['cumulative_volume'] = df['Volume'].cumsum()

        # Avoid division by zero
        if df['cumulative_volume'].iloc[-1] == 0:
            return None

        df['vwap'] = df['cumulative_tpv'] / df['cumulative_volume']

        vwap = df['vwap'].iloc[-1]
        current_price = df['Close'].iloc[-1]
        distance_pct = ((current_price - vwap) / vwap) * 100

        return {
            'symbol': symbol,
            'vwap': round(vwap, 2),
            'current_price': round(current_price, 2),
            'distance_from_vwap_percent': round(distance_pct, 2),
            'above_vwap': distance_pct > 0,
            'data_age': 'live' if is_market_hours() else 'previous_session'
        }

    except Exception as e:
        logger.error(f"VWAP calculation failed for {symbol}: {e}")
        return None
```

**UI Update to show data age:**
```html
<!-- VWAP display with age indicator -->
{% if vwap_data %}
  <div class="vwap-display">
    <span class="label">VWAP:</span>
    <span class="value">${{ vwap_data.vwap }}</span>
    <span class="distance {% if vwap_data.above_vwap %}text-green-600{% else %}text-red-600{% endif %}">
      ({{ vwap_data.distance_from_vwap_percent|floatformat:1 }}% {{ 'above' if vwap_data.above_vwap else 'below' }})
    </span>
    {% if vwap_data.data_age == 'previous_session' %}
      <span class="text-xs text-gray-400">(prev session)</span>
    {% endif %}
  </div>
{% else %}
  <div class="text-xs text-gray-400">VWAP: N/A</div>
{% endif %}
```

**Status:** âœ… Fixed - Added fallbacks for closed markets with clear indicators

---

### 6. News Sentiment Service - Headline Limits & Empty News Handling ğŸ“°

**Issue Identified:**
> Define cutoff for headline count (10 in sample) and throttle strategy. Handle "no headlines" case before invoking Claude.

**Impact:** Medium - Cost control and error handling

**Resolution:**

**Updated News Intelligence Service:**
```python
# strategies/news_intelligence.py (FIXED VERSION)
from anthropic import Anthropic
from django.conf import settings
import json
import logging

logger = logging.getLogger(__name__)

# Configuration constants
MAX_HEADLINES_FOR_SENTIMENT = 10
MIN_HEADLINES_REQUIRED = 1
SENTIMENT_TIMEOUT_SECONDS = 5

def analyze_news_sentiment(symbol: str, headlines: list) -> dict:
    """
    Use Claude Haiku to analyze news sentiment and categorize catalysts.

    Args:
        symbol: Stock ticker
        headlines: List of news headline strings (or dicts with 'headline' key)

    Returns:
        Dict with sentiment_score, catalyst_type, confidence, summary
        Returns neutral sentiment if no headlines or analysis fails
    """
    # Normalize headlines (handle both strings and dicts)
    if not headlines:
        logger.debug(f"No headlines for {symbol} - returning neutral sentiment")
        return {
            'sentiment_score': 0.0,
            'catalyst_type': 'other',
            'confidence': 'low',
            'one_sentence_summary': 'No recent news available'
        }

    normalized_headlines = []
    for item in headlines:
        if isinstance(item, dict):
            normalized_headlines.append(item.get('headline', ''))
        else:
            normalized_headlines.append(str(item))

    # Filter empty headlines
    normalized_headlines = [h for h in normalized_headlines if h.strip()]

    if len(normalized_headlines) < MIN_HEADLINES_REQUIRED:
        logger.debug(f"Insufficient headlines for {symbol} ({len(normalized_headlines)})")
        return {
            'sentiment_score': 0.0,
            'catalyst_type': 'other',
            'confidence': 'low',
            'one_sentence_summary': 'Insufficient news data'
        }

    # Limit headlines to prevent excessive API costs
    headlines_to_analyze = normalized_headlines[:MAX_HEADLINES_FOR_SENTIMENT]

    if len(normalized_headlines) > MAX_HEADLINES_FOR_SENTIMENT:
        logger.info(
            f"Limiting headlines for {symbol}: {len(normalized_headlines)} â†’ {MAX_HEADLINES_FOR_SENTIMENT}"
        )

    headlines_text = '\n'.join([f"- {h}" for h in headlines_to_analyze])

    prompt = f"""Analyze these news headlines for {symbol} and provide:

Headlines:
{headlines_text}

Return JSON with:
1. sentiment_score: -1.0 (very bearish) to +1.0 (very bullish)
2. catalyst_type: earnings|fda|merger|partnership|insider|legal|macro|product|other
3. confidence: low|medium|high (how clear is the catalyst?)
4. one_sentence_summary: Brief explanation (max 15 words)

Example output:
{{"sentiment_score": 0.8, "catalyst_type": "earnings", "confidence": "high", "one_sentence_summary": "Strong Q4 earnings beat with raised guidance"}}

Respond with ONLY the JSON, no other text."""

    try:
        client = Anthropic(api_key=settings.ANTHROPIC_API_KEY)

        response = client.messages.create(
            model=settings.SENTIMENT_MODEL,
            max_tokens=200,
            timeout=SENTIMENT_TIMEOUT_SECONDS,
            messages=[{"role": "user", "content": prompt}]
        )

        # Parse JSON response
        result_text = response.content[0].text.strip()

        # Log raw response at DEBUG level for prompt tuning
        logger.debug(f"Sentiment raw response for {symbol}: {result_text}")

        result = json.loads(result_text)

        # Validate result structure
        required_keys = ['sentiment_score', 'catalyst_type', 'confidence', 'one_sentence_summary']
        if not all(key in result for key in required_keys):
            logger.warning(f"Incomplete sentiment response for {symbol}: {result}")
            raise ValueError("Missing required keys in response")

        # Validate sentiment score range
        if not -1.0 <= result['sentiment_score'] <= 1.0:
            logger.warning(f"Sentiment score out of range for {symbol}: {result['sentiment_score']}")
            result['sentiment_score'] = max(-1.0, min(1.0, result['sentiment_score']))

        return result

    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse sentiment JSON for {symbol}: {e}")
        logger.debug(f"Raw response text: {result_text}")
        return _neutral_sentiment_fallback("Invalid JSON response")

    except Exception as e:
        logger.error(f"Sentiment analysis failed for {symbol}: {e}")
        return _neutral_sentiment_fallback(str(e))

def _neutral_sentiment_fallback(reason='Unknown error'):
    """Return neutral sentiment when analysis fails"""
    return {
        'sentiment_score': 0.0,
        'catalyst_type': 'other',
        'confidence': 'low',
        'one_sentence_summary': f'Analysis unavailable: {reason[:50]}'
    }
```

**Throttling Strategy:**
- Only analyze tracked stocks (not all scanned stocks)
- Limit to 10 headlines max per analysis
- 5-second timeout to prevent hanging
- Cache sentiment results (5-min TTL to allow for new news)

**Cost Impact:**
- Before: Unlimited headlines â†’ unpredictable cost
- After: Max 10 headlines â†’ ~400 input tokens = $0.0001 per analysis

**Status:** âœ… Fixed - Added headline limits, empty news handling, and detailed logging

---

### 7. Auto-Refresh Strategy - Client vs Server Side ğŸ”„

**Issue Identified:**
> Clarify if market context widget refreshes every 60s via JS polling or server-side cached data. Ensure we're not auto-refreshing the entire page.

**Impact:** Low - UX clarity needed

**Resolution:**

**Implementation Strategy: Client-Side AJAX Polling**

```javascript
// static/js/market_context.js (NEW FILE)
/**
 * Market Context Widget - Auto-refresh handler
 *
 * Refreshes market data every 60 seconds via AJAX without reloading page.
 */

class MarketContextWidget {
    constructor(updateIntervalMs = 60000) {  // 60 seconds
        this.updateIntervalMs = updateIntervalMs;
        this.intervalId = null;
        this.endpoint = '/strategies/market-context/';
    }

    async fetchMarketContext() {
        try {
            const response = await fetch(this.endpoint);
            if (!response.ok) {
                throw new Error(`HTTP ${response.status}`);
            }

            const data = await response.json();
            this.updateUI(data);

        } catch (error) {
            console.error('Failed to fetch market context:', error);
            this.showError();
        }
    }

    updateUI(data) {
        // Update SPY
        document.getElementById('spy-change').textContent = `${data.spy_change > 0 ? '+' : ''}${data.spy_change}%`;
        document.getElementById('spy-change').className = data.spy_change > 0 ? 'positive' : 'negative';

        // Update QQQ
        document.getElementById('qqq-change').textContent = `${data.qqq_change > 0 ? '+' : ''}${data.qqq_change}%`;
        document.getElementById('qqq-change').className = data.qqq_change > 0 ? 'positive' : 'negative';

        // Update VIX
        document.getElementById('vix-level').textContent = data.vix_level;

        // Update sentiment badge
        const sentimentBadge = document.getElementById('market-sentiment');
        sentimentBadge.textContent = data.market_sentiment.toUpperCase();
        sentimentBadge.className = `sentiment-badge sentiment-${data.market_sentiment}`;

        // Update timestamp
        document.getElementById('last-updated').textContent = new Date(data.last_updated).toLocaleTimeString('en-US', {
            hour: 'numeric',
            minute: '2-digit',
            hour12: true
        });
    }

    showError() {
        // Show error state without breaking the page
        document.getElementById('market-sentiment').textContent = 'ERROR';
        document.getElementById('market-sentiment').className = 'sentiment-badge sentiment-error';
    }

    start() {
        // Initial load
        this.fetchMarketContext();

        // Set up periodic refresh
        this.intervalId = setInterval(() => {
            this.fetchMarketContext();
        }, this.updateIntervalMs);

        console.log(`Market context auto-refresh started (every ${this.updateIntervalMs / 1000}s)`);
    }

    stop() {
        if (this.intervalId) {
            clearInterval(this.intervalId);
            this.intervalId = null;
            console.log('Market context auto-refresh stopped');
        }
    }
}

// Initialize on page load
document.addEventListener('DOMContentLoaded', () => {
    const widget = new MarketContextWidget(60000);  // 60 seconds
    widget.start();

    // Stop refresh when user navigates away (cleanup)
    window.addEventListener('beforeunload', () => {
        widget.stop();
    });
});
```

**View Endpoint (JSON API):**
```python
# strategies/views.py
from django.http import JsonResponse
from .market_context import get_market_context

def market_context_view(request):
    """
    AJAX endpoint for market context widget.
    Returns JSON, not HTML.
    """
    context = get_market_context()  # Cached 60s on server side

    return JsonResponse({
        'spy_change': context.spy_change,
        'qqq_change': context.qqq_change,
        'vix_level': context.vix_level,
        'es_futures': context.es_futures,
        'nq_futures': context.nq_futures,
        'market_sentiment': context.market_sentiment,
        'last_updated': context.last_updated.isoformat(),
        'data_quality': context.data_quality,
    })
```

**Why This Approach:**
- âœ… No full page reload (AJAX only updates widget)
- âœ… Server-side caching (60s TTL) prevents excessive API calls
- âœ… Client-side polling (60s interval) keeps UI fresh
- âœ… Graceful error handling
- âœ… Automatic cleanup on navigation

**Alternative Considered: Server-Sent Events (SSE)**
- Pros: Real-time push, no polling overhead
- Cons: More complex, requires persistent connections, overkill for 60s refresh
- **Decision: Defer to Phase 4** - AJAX polling is sufficient for Phase 3

**Status:** âœ… Clarified - Client-side AJAX polling with server-side caching

---

### 8. Mobile/Responsive Layout - Card Height Handling ğŸ“±

**Issue Identified:**
> Phase 3 adds VWAP, sentiment chips, etc. - ensure mobile layout handles taller cards.

**Impact:** Low - UX on mobile devices

**Resolution:**

**Responsive Design Updates:**
```html
<!-- strategies/templates/strategies/pre_market_movers.html -->
<!-- Updated tracked stock card with responsive layout -->

<div class="stock-card
            bg-white dark:bg-gray-800
            rounded-lg shadow
            p-4
            flex flex-col
            space-y-3
            min-h-[200px]
            sm:min-h-[180px]">

  <!-- Header: Symbol + Catalyst Badge -->
  <div class="flex items-start justify-between">
    <div>
      <h3 class="text-lg font-bold">{{ stock.symbol }}</h3>
      <p class="text-xs text-gray-500 dark:text-gray-400">{{ stock.company_name|truncatewords:4 }}</p>
    </div>

    {% if stock.catalyst_type %}
      <span class="catalyst-badge catalyst-{{ stock.catalyst_type }}
                   text-xs px-2 py-1 rounded
                   whitespace-nowrap">
        {{ stock.catalyst_type|upper }}
      </span>
    {% endif %}
  </div>

  <!-- Price: Responsive text sizes -->
  <div class="price-display">
    <div class="text-2xl sm:text-3xl font-bold">
      ${{ stock.current_price }}
    </div>
    <div class="flex items-baseline space-x-2">
      <span class="text-lg sm:text-xl font-semibold text-green-600">
        +{{ stock.change_percent }}%
      </span>
      <span class="text-xs text-gray-500">from close</span>
    </div>
  </div>

  <!-- Metrics: Stack on mobile, grid on desktop -->
  <div class="metrics-grid
              grid grid-cols-1
              sm:grid-cols-2
              gap-2
              text-sm">

    <div class="metric">
      <span class="text-gray-500 dark:text-gray-400">RVOL:</span>
      <span class="font-medium">{{ stock.rvol }}x</span>
    </div>

    {% if stock.vwap_data %}
      <div class="metric">
        <span class="text-gray-500 dark:text-gray-400">VWAP:</span>
        <span class="font-medium">${{ stock.vwap_data.vwap }}</span>
        <span class="{% if stock.vwap_data.above_vwap %}text-green-600{% else %}text-red-600{% endif %}">
          ({{ stock.vwap_data.distance_from_vwap_percent|floatformat:1 }}%)
        </span>
      </div>
    {% endif %}
  </div>

  <!-- Sentiment: Full width, responsive bar -->
  {% if stock.sentiment_data %}
    <div class="sentiment-display flex items-center space-x-2">
      <span class="text-xs text-gray-500 w-16">Sentiment:</span>
      <div class="flex-1 h-2 bg-gray-200 dark:bg-gray-700 rounded-full overflow-hidden">
        <div class="h-full transition-all duration-300
                    {% if stock.sentiment_data.score > 0.5 %}bg-green-500
                    {% elif stock.sentiment_data.score > 0 %}bg-green-300
                    {% elif stock.sentiment_data.score < -0.5 %}bg-red-500
                    {% elif stock.sentiment_data.score < 0 %}bg-red-300
                    {% else %}bg-gray-400{% endif %}"
             style="width: {{ ((stock.sentiment_data.score + 1) / 2 * 100)|floatformat:0 }}%">
        </div>
      </div>
      <span class="text-xs font-medium w-10 text-right">
        {{ stock.sentiment_data.score|floatformat:1 }}
      </span>
    </div>
  {% endif %}

  <!-- News: Truncate on mobile -->
  <div class="news-preview text-sm text-gray-600 dark:text-gray-300
              line-clamp-2 sm:line-clamp-3">
    {{ stock.news_summary }}
  </div>

  <!-- Action Button: Full width on mobile -->
  <button class="research-button
                 w-full sm:w-auto
                 px-4 py-2
                 bg-blue-600 hover:bg-blue-700
                 text-white rounded
                 transition-colors">
    Research This
  </button>
</div>
```

**CSS Additions:**
```css
/* static/css/responsive_cards.css */

/* Mobile-first approach */
.stock-card {
  /* Flexible height */
  min-height: 200px;

  /* Prevent content overflow */
  overflow: hidden;
}

/* Tablet and up */
@media (min-width: 640px) {
  .stock-card {
    min-height: 180px;
  }
}

/* Desktop */
@media (min-width: 1024px) {
  .stock-card {
    min-height: 160px;
  }
}

/* Line clamping for news preview */
.line-clamp-2 {
  display: -webkit-box;
  -webkit-line-clamp: 2;
  -webkit-box-orient: vertical;
  overflow: hidden;
}

.line-clamp-3 {
  display: -webkit-box;
  -webkit-line-clamp: 3;
  -webkit-box-orient: vertical;
  overflow: hidden;
}
```

**Testing Checklist:**
- [ ] Test on iPhone SE (375px width) - smallest common viewport
- [ ] Test on iPad (768px) - tablet breakpoint
- [ ] Test on desktop (1920px) - full layout
- [ ] Test in dark mode on all devices
- [ ] Verify sentiment bar doesn't break layout
- [ ] Verify catalyst badges wrap properly

**Status:** âœ… Addressed - Responsive layout with mobile-first design

---

### 9. Regression Testing for Existing Features ğŸ§ª

**Issue Identified:**
> Add explicit regression tests for existing scan features (positive-only filter, pagination). Caching/rate limiting could introduce regressions.

**Impact:** High - Don't break what works

**Resolution:**

**New Test Suite:**
```python
# tests/test_phase_3_regressions.py
from django.test import TestCase, Client
from django.contrib.auth import get_user_model
from strategies.models import TrackedStock

class Phase3RegressionTests(TestCase):
    """
    Regression tests to ensure Phase 3 changes don't break Phase 1/2 features.
    """

    def setUp(self):
        self.client = Client()
        User = get_user_model()
        self.user = User.objects.create_user(username='testuser', password='testpass')
        self.client.login(username='testuser', password='testpass')

    def test_positive_only_filter_still_works(self):
        """Ensure scan only returns positive movers (Phase 1 requirement)"""
        response = self.client.post('/strategies/pre-market-movers/scan/', {
            'universe': 'comprehensive',
            'threshold': '10.0',
            'min_rvol': '3.0'
        })

        self.assertEqual(response.status_code, 302)  # Redirect after scan

        # Check session results
        scan_results = self.client.session.get('scan_results', [])

        # All results should have positive change_percent
        for stock in scan_results:
            self.assertGreaterEqual(
                stock['change_percent'],
                10.0,
                f"{stock['symbol']} has negative change: {stock['change_percent']}"
            )

    def test_pagination_still_works(self):
        """Ensure pagination displays 50 results per page (Phase 2 requirement)"""
        # Create 75 tracked stocks
        for i in range(75):
            TrackedStock.objects.create(
                symbol=f'TEST{i}',
                user=self.user,
                movement_percent=10.0 + i
            )

        # Page 1 should have 50
        response = self.client.get('/strategies/pre-market-movers/?page=1')
        self.assertEqual(len(response.context['tracked_stocks']), 50)

        # Page 2 should have 25
        response = self.client.get('/strategies/pre-market-movers/?page=2')
        self.assertEqual(len(response.context['tracked_stocks']), 25)

    def test_api_toggle_persistence(self):
        """Ensure AI toggle still persists across scans (Phase 2 fix)"""
        # Enable API toggle
        self.client.post('/strategies/pre-market-movers/toggle-api/')

        # Run a scan
        self.client.post('/strategies/pre-market-movers/scan/', {
            'universe': 'comprehensive',
            'threshold': '10.0',
            'min_rvol': '3.0'
        })

        # Check toggle state persisted
        response = self.client.get('/strategies/pre-market-movers/')
        self.assertTrue(
            response.context['api_enabled'],
            "API toggle was reset after scan (regression)"
        )

    def test_cache_doesnt_break_scans(self):
        """Ensure caching doesn't return stale data"""
        # First scan
        response1 = self.client.post('/strategies/pre-market-movers/scan/', {
            'universe': 'comprehensive',
            'threshold': '10.0',
            'min_rvol': '3.0'
        })

        results1 = self.client.session.get('scan_results', [])

        # Second scan with different filters
        response2 = self.client.post('/strategies/pre-market-movers/scan/', {
            'universe': 'comprehensive',
            'threshold': '15.0',  # Different threshold
            'min_rvol': '5.0'      # Different RVOL
        })

        results2 = self.client.session.get('scan_results', [])

        # Results should be different (not cached incorrectly)
        self.assertNotEqual(
            len(results1),
            len(results2),
            "Cache returned same results for different filters"
        )

    def test_rate_limiter_doesnt_block_scans(self):
        """Ensure rate limiter allows scans to complete"""
        import time

        # Run multiple scans quickly
        for i in range(3):
            response = self.client.post('/strategies/pre-market-movers/scan/', {
                'universe': 'nasdaq',  # Smaller universe (99 stocks)
                'threshold': '10.0',
                'min_rvol': '3.0'
            })

            self.assertEqual(response.status_code, 302, f"Scan {i+1} failed")
            time.sleep(1)  # Brief pause between scans

        # All scans should succeed without 429 errors
```

**Run Regression Tests:**
```bash
# Add to test suite
python manage.py test tests.test_phase_3_regressions

# Run before and after Phase 3 implementation
python manage.py test --tag=regression
```

**Status:** âœ… Added - Comprehensive regression test suite

---

### 10. Feature Flags for Gradual Rollout ğŸš©

**Issue Identified:**
> Consider feature flags so we can roll out market-context widget and VWAP in dev/staging before enabling on main instance.

**Impact:** Low - Deployment safety

**Resolution:**

**Feature Flag System:**
```python
# config/feature_flags.py (NEW FILE)
"""
Feature flags for Phase 3 rollout.

Usage:
    from config.feature_flags import is_feature_enabled

    if is_feature_enabled('market_context_widget'):
        # Show market context widget
"""

from django.conf import settings
import os

# Feature flag definitions
FEATURES = {
    'market_context_widget': {
        'default': False,
        'description': 'Market Context Widget (SPY, QQQ, VIX, futures)',
    },
    'vwap_analysis': {
        'default': False,
        'description': 'VWAP calculation and display',
    },
    'news_sentiment': {
        'default': False,
        'description': 'AI-powered news sentiment scoring',
    },
    'catalyst_categorization': {
        'default': False,
        'description': 'Automatic catalyst type tagging',
    },
    'redis_rate_limiter': {
        'default': False,
        'description': 'Redis-backed rate limiter (multi-process)',
    },
}

def is_feature_enabled(feature_name: str) -> bool:
    """
    Check if a feature is enabled.

    Reads from environment variable: FEATURE_{NAME}_ENABLED
    Falls back to default in FEATURES dict.

    Example:
        FEATURE_MARKET_CONTEXT_WIDGET_ENABLED=true
    """
    if feature_name not in FEATURES:
        raise ValueError(f"Unknown feature: {feature_name}")

    env_var = f"FEATURE_{feature_name.upper()}_ENABLED"
    env_value = os.getenv(env_var)

    if env_value is not None:
        return env_value.lower() in ('true', '1', 'yes', 'on')

    return FEATURES[feature_name]['default']

def get_all_features() -> dict:
    """Get all features with their current status"""
    return {
        name: {
            'enabled': is_feature_enabled(name),
            'description': config['description']
        }
        for name, config in FEATURES.items()
    }
```

**Environment Configuration:**
```bash
# .env.development
FEATURE_MARKET_CONTEXT_WIDGET_ENABLED=true
FEATURE_VWAP_ANALYSIS_ENABLED=true
FEATURE_NEWS_SENTIMENT_ENABLED=false  # Not ready yet
FEATURE_CATALYST_CATEGORIZATION_ENABLED=false
FEATURE_REDIS_RATE_LIMITER_ENABLED=false  # Use in-memory for dev

# .env.staging
FEATURE_MARKET_CONTEXT_WIDGET_ENABLED=true
FEATURE_VWAP_ANALYSIS_ENABLED=true
FEATURE_NEWS_SENTIMENT_ENABLED=true  # Test in staging
FEATURE_CATALYST_CATEGORIZATION_ENABLED=true
FEATURE_REDIS_RATE_LIMITER_ENABLED=true  # Test multi-process

# .env.production
FEATURE_MARKET_CONTEXT_WIDGET_ENABLED=false  # Start disabled
FEATURE_VWAP_ANALYSIS_ENABLED=false
FEATURE_NEWS_SENTIMENT_ENABLED=false
FEATURE_CATALYST_CATEGORIZATION_ENABLED=false
FEATURE_REDIS_RATE_LIMITER_ENABLED=true  # Must use Redis in prod
```

**Template Usage:**
```html
<!-- strategies/templates/strategies/pre_market_movers.html -->
{% load feature_flags %}

{% if 'market_context_widget'|is_feature_enabled %}
  <!-- Market Context Widget -->
  <div class="market-context-widget">
    ...
  </div>
{% endif %}

{% if 'vwap_analysis'|is_feature_enabled %}
  <!-- VWAP Display -->
  <div class="vwap-display">
    ...
  </div>
{% endif %}
```

**Admin Dashboard:**
```python
# strategies/views.py
def feature_flags_admin(request):
    """Admin page to view/toggle feature flags"""
    from config.feature_flags import get_all_features

    if not request.user.is_staff:
        return HttpResponseForbidden()

    features = get_all_features()

    return render(request, 'admin/feature_flags.html', {
        'features': features
    })
```

**Rollout Plan:**
1. **Week 1:** Enable in dev â†’ test locally
2. **Week 2:** Enable in staging â†’ QA testing
3. **Week 3:** Enable in production for 10% of users (A/B test)
4. **Week 4:** Enable for 100% if metrics look good

**Status:** âœ… Added - Feature flag system with environment-based config

---

### 11. Watch-Outs & Logging Improvements ğŸ“

**Issue Identified:**
> Log malformed JSON from sentiment prompts at DEBUG level for refinement. Document Redis deployment steps for Apache setups.

**Impact:** Low - Operational improvements

**Resolution:**

#### Enhanced Logging Configuration:
```python
# config/settings.py - Logging configuration
LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        'verbose': {
            'format': '{levelname} {asctime} {module} {message}',
            'style': '{',
        },
    },
    'handlers': {
        'file': {
            'level': 'DEBUG',
            'class': 'logging.FileHandler',
            'filename': 'logs/phase3_debug.log',
            'formatter': 'verbose',
        },
        'console': {
            'level': 'INFO',
            'class': 'logging.StreamHandler',
            'formatter': 'verbose',
        },
    },
    'loggers': {
        'strategies': {
            'handlers': ['file', 'console'],
            'level': 'DEBUG',
            'propagate': False,
        },
        'strategies.news_intelligence': {
            'handlers': ['file'],
            'level': 'DEBUG',  # Capture all sentiment API responses
        },
        'strategies.api_monitoring': {
            'handlers': ['file', 'console'],
            'level': 'INFO',
        },
    },
}
```

#### Redis Deployment Documentation:
```markdown
# docs/REDIS_DEPLOYMENT.md (NEW FILE)

# Redis Deployment Guide for Phase 3

## Development (macOS/Linux)

### Install Redis
```bash
# macOS
brew install redis
brew services start redis

# Linux (Ubuntu/Debian)
sudo apt-get update
sudo apt-get install redis-server
sudo systemctl start redis
sudo systemctl enable redis

# Verify
redis-cli ping  # Should return "PONG"
```

### Configure Django
```bash
# .env
REDIS_URL=redis://localhost:6379/0
```

## Production (Apache + mod_wsgi)

### Install Redis Server
```bash
# Linux
sudo apt-get install redis-server

# Configure persistence
sudo nano /etc/redis/redis.conf
# Set: appendonly yes
# Set: appendfsync everysec

sudo systemctl restart redis
```

### Install Python Redis Client
```bash
# In virtual environment
pip install redis django-redis
```

### Configure Django Settings
```python
# config/settings.py
import os

CACHES = {
    'default': {
        'BACKEND': 'django_redis.cache.RedisCache',
        'LOCATION': os.getenv('REDIS_URL', 'redis://127.0.0.1:6379/0'),
        'OPTIONS': {
            'CLIENT_CLASS': 'django_redis.client.DefaultClient',
            'SOCKET_CONNECT_TIMEOUT': 5,
            'SOCKET_TIMEOUT': 5,
            'CONNECTION_POOL_KWARGS': {
                'max_connections': 50,
                'retry_on_timeout': True,
            },
        },
        'KEY_PREFIX': 'picker',
        'TIMEOUT': 300,
    }
}
```

### Apache Configuration
```apache
# /etc/apache2/sites-available/picker.conf

<VirtualHost *:80>
    ServerName your-domain.com

    # ... existing Apache config ...

    # Ensure Redis is accessible
    # No special Apache config needed - Django handles Redis connections

    # Optional: Monitor Redis health
    Alias /redis-health /path/to/health_check.html
</VirtualHost>
```

### Environment Variables
```bash
# /path/to/picker/.env
REDIS_URL=redis://127.0.0.1:6379/0

# Optional: Redis password (recommended for production)
# REDIS_URL=redis://:password@127.0.0.1:6379/0
```

### Troubleshooting

**Issue:** Django can't connect to Redis
```bash
# Check Redis is running
sudo systemctl status redis

# Check port 6379 is listening
sudo netstat -tlnp | grep 6379

# Test connection
redis-cli ping
```

**Issue:** Permission denied
```bash
# Check Redis socket permissions
ls -la /var/run/redis/

# Ensure www-data (Apache user) can access Redis
sudo usermod -aG redis www-data
```

**Issue:** Cache not working
```python
# Test in Django shell
from django.core.cache import cache
cache.set('test', 'hello', 60)
print(cache.get('test'))  # Should print 'hello'
```

### Monitoring Redis
```bash
# Monitor Redis in real-time
redis-cli monitor

# Check memory usage
redis-cli info memory

# Check connected clients
redis-cli client list
```

### Backup Strategy
```bash
# Redis auto-saves to /var/lib/redis/dump.rdb
# Backup daily
0 2 * * * /usr/bin/cp /var/lib/redis/dump.rdb /backups/redis-$(date +\%Y\%m\%d).rdb
```

## Fallback to File-Based Cache

If Redis fails, Django automatically falls back to file-based cache:

```python
# Fallback config (automatic in settings.py)
if not os.getenv('REDIS_URL'):
    CACHES = {
        'default': {
            'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',
            'LOCATION': '/tmp/django_cache',
            'TIMEOUT': 300,
        }
    }
```

**Performance Impact:**
- Redis: <1ms cache lookups
- File cache: 5-20ms cache lookups
- No cache: 100-2000ms API calls

**Recommendation:** Use Redis in production for best performance.
```

**Status:** âœ… Documented - Redis deployment guide added, logging enhanced

---

## Summary of Changes

### All Codex Concerns Addressed âœ…

| Issue | Status | Summary |
|-------|--------|---------|
| 1. Rate limiter multi-process | âœ… Fixed | Added Redis-backed rate limiter for production |
| 2. Cache arg hashing | âœ… Fixed | Stable JSON serialization with sorted keys |
| 3. YFinance throughput | âœ… Added | API call monitoring with alert system |
| 4. Market context optimization | âœ… Optimized | Using fast_info with graceful degradation |
| 5. VWAP fallbacks | âœ… Fixed | Handles closed markets with previous session data |
| 6. News sentiment limits | âœ… Fixed | Max 10 headlines, empty news handling |
| 7. Auto-refresh strategy | âœ… Clarified | Client-side AJAX with server caching |
| 8. Mobile layout | âœ… Addressed | Responsive design with mobile-first approach |
| 9. Regression testing | âœ… Added | Comprehensive test suite for existing features |
| 10. Feature flags | âœ… Added | Environment-based gradual rollout system |
| 11. Logging & deployment | âœ… Documented | Redis deployment guide + enhanced logging |

### Updated Implementation Priorities

**Wave 1 remains P0** - Infrastructure is critical and now more robust:
- âœ… Redis-backed rate limiter (production-ready)
- âœ… Stable caching decorator (handles complex args)
- âœ… API monitoring system (detect rate limits early)

**Wave 2 ready to implement** - Quick wins optimized:
- âœ… Market context using fast_info (2-3x faster)
- âœ… VWAP with fallbacks (works outside market hours)

**Wave 3 ready to implement** - AI features with safeguards:
- âœ… News sentiment with headline limits (cost-controlled)
- âœ… Empty news handling (no wasted API calls)

**All waves have regression protection:**
- âœ… Test suite ensures Phase 1/2 features don't break
- âœ… Feature flags allow gradual rollout
- âœ… Monitoring catches issues early

---

**Document Version:** 1.2 (Final Codex Sign-Off)
**Last Updated:** October 30, 2025
**Next Review:** After Wave 1 implementation
**Status:** âœ… APPROVED - READY FOR IMPLEMENTATION

---

## Final Implementation Notes

### Timezone Handling Decision âœ…

**Codex Note Addressed:** `is_market_hours()` timezone conversion

**Decision:** Use `zoneinfo` (Python 3.9+ standard library)
- **Why:** Standard library (no external dependencies)
- **Why not pytz:** Being phased out in favor of zoneinfo
- **Why not pendulum:** Overkill for this use case, adds dependency

**Implementation:**
```python
from zoneinfo import ZoneInfo

et_tz = ZoneInfo('America/New_York')
now_et = datetime.now(et_tz)
```

**Additional Improvements:**
- Added weekend check (markets closed Saturday/Sunday)
- Handles daylight saving time automatically (EDT/EST)
- No external dependencies required

**Python Version Requirement:** Python 3.9+ (current project uses Python 3.11)

---

**Codex Sign-Off:** âœ… APPROVED

This document provides:
- âœ… Comprehensive implementation plans for all features
- âœ… Detailed code examples and architecture decisions
- âœ… Cost analysis and performance projections
- âœ… Risk assessment and mitigation strategies
- âœ… Clear success metrics and testing strategy
- âœ… 20 specific questions for Codex to address

**Recommended Next Steps:**
1. Codex reviews this document and provides feedback
2. Address any concerns or questions from Codex
3. Prioritize features based on Codex input
4. Begin Wave 1 implementation (Infrastructure)
